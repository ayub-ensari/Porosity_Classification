{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "843d78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import time\n",
    "import regex as re\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92275cb",
   "metadata": {},
   "source": [
    "> The \"pipreqs\" generates a puthon packages requirements text file based on the imports in the current jupyter notebook.\n",
    "\n",
    "> The required packages could be installed at once using pip and requirements.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dcd8d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pipreqs in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (0.4.11)\n",
      "Requirement already satisfied: yarg in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from pipreqs) (0.1.9)\n",
      "Requirement already satisfied: docopt in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from pipreqs) (0.6.2)\n",
      "Requirement already satisfied: requests in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from yarg->pipreqs) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from requests->yarg->pipreqs) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from requests->yarg->pipreqs) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from requests->yarg->pipreqs) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from requests->yarg->pipreqs) (1.26.12)\n",
      "Requirement already satisfied: nbconvert in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: defusedxml in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (0.6.7)\n",
      "Requirement already satisfied: lxml in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (4.9.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (2.1.1)\n",
      "Requirement already satisfied: nbformat>=5.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (5.4.0)\n",
      "Requirement already satisfied: tinycss2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (1.1.1)\n",
      "Requirement already satisfied: traitlets>=5.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (5.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (2.0.4)\n",
      "Requirement already satisfied: pygments>=2.4.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (2.13.0)\n",
      "Requirement already satisfied: bleach in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (5.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (4.12.0)\n",
      "Requirement already satisfied: packaging in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (21.3)\n",
      "Requirement already satisfied: jinja2>=3.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbconvert) (3.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert) (3.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (304)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (7.3.5)\n",
      "Requirement already satisfied: nest-asyncio in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (1.5.5)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbformat>=5.1->nbconvert) (4.14.0)\n",
      "Requirement already satisfied: fastjsonschema in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from nbformat>=5.1->nbconvert) (2.16.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from beautifulsoup4->nbconvert) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from bleach->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from packaging->nbconvert) (3.0.9)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (22.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (0.4)\n",
      "Requirement already satisfied: pyzmq>=23.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (23.2.1)\n",
      "Requirement already satisfied: tornado>=6.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pipreqs\n",
    "!pip install nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865269e3",
   "metadata": {},
   "source": [
    "> So what we’ve done here is converted our notebook into a .py file in a new directory called reqs, then run pipreqs in the new directory. The reason for this is that pipreqs only works on .py files and I can’t seem to get it to work when there are other files in the folder. The requirements.txt will be generated in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d426ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 1_Data_Collection.ipynb to script\n",
      "[NbConvertApp] Writing 17131 bytes to reqs\\1_Data_Collection.py\n",
      "INFO: Successfully saved requirements file in D:\\UoH_PhD_Exp\\Projects\\Porosity\\requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --output-dir=\"./reqs\" --to script 1_Data_Collection.ipynb\n",
    "!cd reqs\n",
    "!pipreqs --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "343675a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib==3.5.3\n",
      "numpy==1.23.2\n",
      "opencv_python==4.6.0.66\n",
      "pandas==1.4.3\n",
      "regex==2022.8.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Successfully output requirements\n"
     ]
    }
   ],
   "source": [
    "!pipreqs --print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea32ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.7.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: matplotlib==3.5.3 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 2)) (3.5.3)\n",
      "Requirement already satisfied: numpy==1.23.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 3)) (1.23.2)\n",
      "Requirement already satisfied: opencv_python==4.6.0.66 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 4)) (4.6.0.66)\n",
      "Requirement already satisfied: pandas==1.4.3 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 5)) (1.4.3)\n",
      "Requirement already satisfied: regex==2022.8.17 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 6)) (2022.8.17)\n",
      "Requirement already satisfied: scikit_learn==1.1.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: seaborn==0.11.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Collecting tensorflow==2.9.1\n",
      "  Using cached tensorflow-2.9.1-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Requirement already satisfied: tensorflow_gpu==2.7.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from -r requirements.txt (line 10)) (2.7.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (4.37.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from pandas==1.4.3->-r requirements.txt (line 5)) (2022.2.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from scikit_learn==1.1.2->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from scikit_learn==1.1.2->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from scikit_learn==1.1.2->-r requirements.txt (line 7)) (1.9.1)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (0.26.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (4.3.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\uoh_phd_exp\\my_ve_env\\lib\\site-packages (from tensorflow==2.9.1->-r requirements.txt (line 9)) (1.47.0)\n",
      "INFO: pip is looking at multiple versions of seaborn to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting seaborn==0.11.2\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "INFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scikit_learn==1.1.2\n",
      "  Using cached scikit_learn-1.1.2-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "INFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting regex==2022.8.17\n",
      "  Using cached regex-2022.8.17-cp39-cp39-win_amd64.whl (263 kB)\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas==1.4.3\n",
      "  Using cached pandas-1.4.3-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv_python==4.6.0.66\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy==1.23.2\n",
      "  Using cached numpy-1.23.2-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting matplotlib==3.5.3\n",
      "  Using cached matplotlib-3.5.3-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting keras==2.7.0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested keras==2.7.0\n",
      "    tensorflow 2.9.1 depends on keras<2.10.0 and >=2.9.0rc0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install -r requirements.txt (line 9) and keras==2.7.0 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0493d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]\n",
      "pandas: 1.4.3\n",
      "numpy: 1.23.2\n",
      "seaborn: 0.11.2\n",
      "sklearn: 1.1.2\n",
      "tensorflow: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "print('Python: {}'.format(sys.version))\n",
    "# pandas\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "# numpy\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "# seaborn\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "# scikit-learn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "# Tensorflow-GPU\n",
    "print('tensorflow: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33aea02",
   "metadata": {},
   "source": [
    "## Define functions - Read Images\n",
    "\n",
    ">The load_all_image_path function read all the files in the given directory and return a list of all file's path and the labels. Labels are merely the name of the image file on hard-drive. An example of files is\n",
    "\n",
    "> './Build2\\\\2020-03-08_13-14-42_layer_02955.jpg',\n",
    "\n",
    "> Whereas, a label is\n",
    "\n",
    "> '2020-03-08_13-14-42_layer_02955.jpg',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dac5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_image_path(img_dir):\n",
    "    \n",
    "    #img_dir = \"./Build2\" # Enter Directory of all images\n",
    "    img_labels = []\n",
    "    for(_, _, filenames) in walk(img_dir):\n",
    "        img_labels.extend(filenames)\n",
    "        break\n",
    "    data_path = os.path.join(img_dir,'*g')\n",
    "    files = glob.glob(data_path)\n",
    "    return files, img_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cfb2e2",
   "metadata": {},
   "source": [
    "## Loading Images\n",
    "> Read all the directorries in the given folder. It returns images paths and their labels.\n",
    "\n",
    "> __img_dir_paths__ = All Image's directory path/address.\n",
    "<br>\n",
    "__img_names__ = All Image's names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir_paths, img_names = load_all_image_path(\"D:/UoH_PhD_Exp/Data/Build2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c326f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2922\n",
      "<class 'list'>\n",
      "2922\n"
     ]
    }
   ],
   "source": [
    "def var_info(var):\n",
    "    print(type(var))\n",
    "    print(len(var))\n",
    "var_info(img_dir_paths)\n",
    "var_info(img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac27834",
   "metadata": {},
   "source": [
    "> Since out of all the images the first 1250 layers/images are relevant to our builts. That's why only the first 1250 are considered. For B1 and B2, the effective printing layers are 244-1242 and for B3 219-1217. But for simplicity, uniformity and avoiding complexity, first 1250 layers are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dfdfde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "img_dir_paths = img_dir_paths[0:1250]\n",
    "img_names = img_names[0:1250]\n",
    "print(len(img_dir_paths))\n",
    "print(len(img_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071c1ea",
   "metadata": {},
   "source": [
    "### Layers with porosity\n",
    "#### Old Labels\n",
    "\n",
    "> Followig are the index numbers of porosity images from three cylinders, B1, B2 & B3. The old labels were based on the CAD file\n",
    "information. Whereas, XCT analysis of cylinders revealed that some images were wrongly labelled as pores. The new labels remove the wrong image indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bda4969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_prosity_index = list(range(311,380)) + list(range(537, 554)) + list(range(628, 663)) + list(range(832, 862)) + list(range(936, 937)) + list(range(940, 953)) + list(range(1011, 1078)) + list(range(1145, 1152))\n",
    "b2_prosity_index = list(range(311,380)) + list(range(428, 463)) + list(range(531, 560)) + list(range(640, 654)) + list(range(737, 753))\n",
    "b3_prosity_index = list(range(420,456)) + list(range(519, 546)) + list(range(619, 634)) + list(range(719, 736)) + list(range(819, 827)) + list(range(919, 923))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4facb256",
   "metadata": {},
   "source": [
    "#### New Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d57279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_remove_index = [311,312,313,318,320,325,326,335,340,366,369,374,375,376,537,538,539,540,541,542,543,544,545,546,547,\n",
    "                       548,549,550,551,552,553,628,629,630,633,640,641,642,643,646,647,648,649,650,651,653,654,656,657,659,\n",
    "                      661,662,833,832,833,834,835,836,837,838,840,842,843,844,845,846,847,849,850,851,852,853,855,857,936,\n",
    "                       940,947,949,950,952,1011,1011,1012,1014,1018,1019,1020,1029,1030,1045,1075,1145,1146,1147,1148,1149,\n",
    "                       1150,1151]\n",
    "\n",
    "\n",
    "b2_remove_index = [320,324,429,430,431,432,433,434,437,450,451,452,456,459,462,531,532,533,534,535,536,537,\n",
    "                      538,539,540,541,542,544,545,548,549,550,554,559,640,641,642,643,644,645,646,647,651,737,\n",
    "                      740,741,742,743,744,745,748,750,751,752]\n",
    "\n",
    "\n",
    "b3_remove_index = [420,423,425,436,439,442,449,453,519,521,522,533,534,538,541,542,543,620,621,622,627,629,\n",
    "                      630,631,632,721,723,724,727,728,729,733,735,819,820,821,822,826,919,920,921,922]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e850fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of porosity images in B1 Cylinder:  143 \n",
      "\n",
      "[314, 315, 316, 317, 319, 321, 322, 323, 324, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 370, 371, 372, 373, 377, 378, 379, 631, 632, 634, 635, 636, 637, 638, 639, 644, 645, 652, 655, 658, 660, 839, 841, 848, 854, 856, 858, 859, 860, 861, 941, 942, 943, 944, 945, 946, 948, 951, 1013, 1015, 1016, 1017, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1076, 1077]\n"
     ]
    }
   ],
   "source": [
    "b1_prosity_index = [x for x in b1_prosity_index if x not in b1_remove_index]\n",
    "print(\"Number of porosity images in B1 Cylinder: \",len(b1_prosity_index), \"\\n\")\n",
    "print(b1_prosity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59ae6f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of porosity images in B2 Cylinder:  109 \n",
      "\n",
      "[311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 428, 435, 436, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 453, 454, 455, 457, 458, 460, 461, 543, 546, 547, 551, 552, 553, 555, 556, 557, 558, 648, 649, 650, 652, 653, 738, 739, 746, 747, 749]\n"
     ]
    }
   ],
   "source": [
    "b2_prosity_index = [x for x in b2_prosity_index if x not in b2_remove_index]\n",
    "print(\"Number of porosity images in B2 Cylinder: \",len(b2_prosity_index), \"\\n\")\n",
    "print(b2_prosity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2b57292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of porosity images in B3 Cylinder:  65 \n",
      "\n",
      "[421, 422, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 440, 441, 443, 444, 445, 446, 447, 448, 450, 451, 452, 454, 455, 520, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 535, 536, 537, 539, 540, 544, 545, 619, 623, 624, 625, 626, 628, 633, 719, 720, 722, 725, 726, 730, 731, 732, 734, 823, 824, 825]\n"
     ]
    }
   ],
   "source": [
    "b3_prosity_index = [x for x in b3_prosity_index if x not in b3_remove_index]\n",
    "print(\"Number of porosity images in B3 Cylinder: \",len(b3_prosity_index), \"\\n\")\n",
    "print(b3_prosity_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e6002",
   "metadata": {},
   "source": [
    "### Image selection and Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e0a54",
   "metadata": {},
   "source": [
    "> The following function receives a chunk of image's path and their corresponding labels. Not all the images in Build2 are relevant to our cylinders. Out of total 2922 images, only 963 images relevant to our 3d objects. Three cylinders names as B1, B2, B3 were printed. Images from 243 to 1243 are related to B1 and B2 cylinders. Whereas, B3 cylinder related images are ranges from 218 to 1218. \n",
    "<br><br><br>\n",
    "Firstly, the images were read into a numpy array. The image dimensions are __height = 2600 and Width = 1420__. Each image is then cropped into three small sections. __Height=1250-1440 and width=650-1100__ is firstly croped from the whole powder bed image.  <br> <br>\n",
    "The cropped image is further is divided into three parts, each containg the image of a cylinder[B1,B2,B3]. The coordinates of __B1=[h:0-190, w:0-150]__, __B2 = [h:0-190, w:150-300]__ , __B3 = [h:0-190, w:300-450]__. The three images were then stored in different folders on the hard-drive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd447a",
   "metadata": {},
   "source": [
    "> The __crop_save_images__ function read images from hard drive and crop out B1, B2, and B3 cylinders into individual images. It also labels the images. The label consisted of \n",
    "__label = Porosity_flag +  cylinder name + layer number__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64da3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_save_images(files, directory, labels):\n",
    "   \n",
    "    for f1,lab in zip(files,labels):\n",
    "        #F1 = File path.\n",
    "        #lab = Image label\n",
    "        #print(\"F1: \" + str(f1))\n",
    "        #print(\"Lab: \" +  str(lab))\n",
    "        ########## read image\n",
    "        orig_img = cv2.imread(f1)\n",
    "\n",
    "        ########### crop image\n",
    "        img = orig_img[1250:1440, 650:1100]\n",
    "        img1 = img[0:190,0:150]\n",
    "        img2 = img[0:190,150:300]\n",
    "        img3 = img[0:190,300:450]\n",
    "\n",
    "        ########### Label Image\n",
    "        \n",
    "        tt = lab[:-4].split('_')\n",
    "        #tt = layer number\n",
    "        #print(tt[3])\n",
    "        layer_no = int(tt[3])\n",
    "        \n",
    "        if (layer_no in b1_prosity_index):\n",
    "            img_name_b1 = \"1_B1_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(layer_no)\n",
    "            #print(\"True\")\n",
    "        else:\n",
    "            img_name_b1 = \"0_B1_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(\"False\")\n",
    "            \n",
    "        if (layer_no in b2_prosity_index):\n",
    "            img_name_b2 = \"1_B2_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(layer_no)\n",
    "            #print(\"True\")\n",
    "        else:\n",
    "            img_name_b2 = \"0_B2_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(\"False\")\n",
    "            \n",
    "        if (layer_no in b3_prosity_index):\n",
    "            img_name_b3 = \"1_B3_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(layer_no)\n",
    "            #print(\"True\")\n",
    "        else:\n",
    "            img_name_b3 = \"0_B3_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(\"False\")\n",
    "        ########### store image\n",
    "        if(layer_no>243 and layer_no<1243):\n",
    "            img_name = directory[0] + img_name_b1\n",
    "            matplotlib.image.imsave(img_name, img1)\n",
    "        \n",
    "            img_name = directory[1] + img_name_b2\n",
    "            matplotlib.image.imsave(img_name, img2)\n",
    "        if(layer_no>218 and layer_no<1218):\n",
    "            img_name = directory[2] + img_name_b3\n",
    "            matplotlib.image.imsave(img_name, img3)\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88663e",
   "metadata": {},
   "source": [
    "> The __crop_save_images_with_poreSize__ function is exactly the saem as crop_save_images function except it also add the pore sizeinformation to the image's label. \n",
    "__label = Porosity_flag +  cylinder name + layer number + Pore Size__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50ddd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_save_images_with_poreSize(files, directory, labels):\n",
    "    \n",
    "    for f1,lab in zip(files,labels):\n",
    "        #F1 = File path.\n",
    "        #lab = Image label\n",
    "        #print(\"F1: \" + str(f1))\n",
    "        #print(\"Lab: \" +  str(lab))\n",
    "        ########## read image\n",
    "        orig_img = cv2.imread(f1)\n",
    "\n",
    "        ########### crop image\n",
    "        img = orig_img[1250:1440, 650:1100]\n",
    "        img1 = img[0:190,0:150]\n",
    "        img2 = img[0:190,150:300]\n",
    "        img3 = img[0:190,300:450]\n",
    "\n",
    "        ########### Label Image\n",
    "        \n",
    "        tt = lab[:-4].split('_')\n",
    "        #tt = layer number\n",
    "        #print(tt[3])\n",
    "        layer_no = int(tt[3])\n",
    "        #########################################################################################-----B1\n",
    "        if (layer_no in b1_prosity_index):\n",
    "            ############################################################### 2mm\n",
    "            if(layer_no >= 314 and layer_no <=379) or (layer_no >= 1013 and layer_no <=1077):\n",
    "                img_name_b1 = \"1_B1_Layer_\"+str(layer_no)+\"_2mm\"+\".jpg\"\n",
    "            ############################################################### 1mm\n",
    "            elif (layer_no >= 631 and layer_no <=660) or (layer_no >= 839 and layer_no <=861):\n",
    "                img_name_b1 = \"1_B1_Layer_\"+str(layer_no)+\"_1mm\"+\".jpg\"\n",
    "            ############################################################### 0.5mm\n",
    "            elif (layer_no >= 941 and layer_no <=951):\n",
    "                img_name_b1 = \"1_B1_Layer_\"+str(layer_no)+\"_05mm\"+\".jpg\"\n",
    "        else:\n",
    "            img_name_b1 = \"0_B1_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(\"False\")\n",
    "        #########################################################################################-----B2\n",
    "        if (layer_no in b2_prosity_index):\n",
    "            ############################################################### 2mm\n",
    "            if(layer_no >= 311 and layer_no <=379):\n",
    "                img_name_b2 = \"1_B2_Layer_\"+str(layer_no)+\"_2mm\"+\".jpg\"\n",
    "            ############################################################### 1mm\n",
    "            elif (layer_no >= 428 and layer_no <=461):\n",
    "                img_name_b2 = \"1_B2_Layer_\"+str(layer_no)+\"_1mm\"+\".jpg\"\n",
    "            ############################################################### 0.8mm\n",
    "            elif (layer_no >= 543 and layer_no <=558):\n",
    "                img_name_b2 = \"1_B2_Layer_\"+str(layer_no)+\"_08mm\"+\".jpg\"\n",
    "            ############################################################### 0.5mm\n",
    "            elif (layer_no >= 648 and layer_no <=653):\n",
    "                img_name_b2 = \"1_B2_Layer_\"+str(layer_no)+\"_05mm\"+\".jpg\"\n",
    "            ############################################################### 0.4mm\n",
    "            elif (layer_no >= 738 and layer_no <=749):\n",
    "                img_name_b2 = \"1_B2_Layer_\"+str(layer_no)+\"_04mm\"+\".jpg\"\n",
    "            \n",
    "        else:\n",
    "            img_name_b2 = \"0_B2_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(\"False\")\n",
    "        #########################################################################################-----B3\n",
    "        if (layer_no in b3_prosity_index):\n",
    "            ############################################################### 1mm\n",
    "            if(layer_no >= 421 and layer_no <=455):\n",
    "                img_name_b3 = \"1_B3_Layer_\"+str(layer_no)+\"_1mm\"+\".jpg\"\n",
    "            ############################################################### 0.8mm\n",
    "            elif (layer_no >= 520 and layer_no <=545):\n",
    "                img_name_b3 = \"1_B3_Layer_\"+str(layer_no)+\"_08mm\"+\".jpg\"\n",
    "            ############################################################### 0.5mm\n",
    "            elif (layer_no >= 619 and layer_no <=633):\n",
    "                img_name_b3 = \"1_B3_Layer_\"+str(layer_no)+\"_05mm\"+\".jpg\"\n",
    "            ############################################################### 0.4mm\n",
    "            elif (layer_no >= 719 and layer_no <=734):\n",
    "                img_name_b3 = \"1_B3_Layer_\"+str(layer_no)+\"_04mm\"+\".jpg\"\n",
    "            ############################################################### 0.2mm\n",
    "            elif (layer_no >= 823 and layer_no <=825):\n",
    "                img_name_b3 = \"1_B3_Layer_\"+str(layer_no)+\"_02mm\"+\".jpg\"\n",
    "            #print(layer_no)\n",
    "            #print(\"True\")\n",
    "        else:\n",
    "            img_name_b3 = \"0_B3_Layer_\"+str(layer_no)+\".jpg\"\n",
    "            #print(\"False\")\n",
    "        ########### store image\n",
    "        if(layer_no>243 and layer_no<1243):\n",
    "            img_name = directory[0] + img_name_b1\n",
    "            matplotlib.image.imsave(img_name, img1)\n",
    "        \n",
    "            img_name = directory[1] + img_name_b2\n",
    "            matplotlib.image.imsave(img_name, img2)\n",
    "        if(layer_no>218 and layer_no<1218):\n",
    "            img_name = directory[2] + img_name_b3\n",
    "            matplotlib.image.imsave(img_name, img3)\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a3c92",
   "metadata": {},
   "source": [
    "> The cropped images are stored separately on B1, B2 and B3 directories on hard drive. The following code insure that the directories exsists. If not already exsists, it will create these folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d471384",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\"D:/UoH_PhD_Exp/Data/Crop_images/B1/\", \"D:/UoH_PhD_Exp/Data/Crop_images/B2/\", \"D:/UoH_PhD_Exp/Data/Crop_images/B3/\"]\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa551fb4",
   "metadata": {},
   "source": [
    "#### Remove all the old files in B1, B2 & B3 folder\n",
    "> Since we will be cropping images many times depedning upon the task at hand. Therefore, it is necessary to delete the old cropped images before saving the new cropped images. The following code empty the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d49d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    files = glob.glob(os.path.join(directory,\"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860f35b",
   "metadata": {},
   "source": [
    "#### Cropping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a682347",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_save_images(img_dir_paths[217:1206] ,directories, img_names[217:1206]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ceab8",
   "metadata": {},
   "source": [
    "##  Loading cropped images\n",
    "### B1 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b9e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 150, 3)\n",
      "0\n",
      "b1_1000\n",
      "[0 1] [826 137]\n",
      "(963, 190, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "files, labels = load_all_image_path(\"D:/UoH_PhD_Exp/Data/Crop_images/B1/\")\n",
    "#print(labels[0])\n",
    "#print(files[0])\n",
    "data = []\n",
    "b1_labels = list()\n",
    "b1_layer_numbers = list()\n",
    "for f1, lab in zip(files, labels):\n",
    "    #print(\"lab:\" + lab)\n",
    "    layer_num = re.search('Layer_(.+?).jpg', lab).group(1)\n",
    "    b1_layer_numbers.append(\"b1_\"+str(layer_num))\n",
    "    b1_labels.append(int(lab[0]))\n",
    "    img = cv2.imread(f1)\n",
    "    ######### Convert to Images to grey scale.\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    data.append(img)\n",
    "b1_images = np.array(data)\n",
    "print(b1_images[0].shape)\n",
    "print(b1_labels[0])\n",
    "print(b1_layer_numbers[0])\n",
    "(unique, counts) = np.unique(b1_labels, return_counts=True)\n",
    "print(unique, counts)\n",
    "print(b1_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71eafcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 150, 3)\n",
      "0\n",
      "b2_1000\n",
      "[0 1] [854 109]\n",
      "(963, 190, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "files, labels = load_all_image_path(\"D:/UoH_PhD_Exp/Data/Crop_images/B2/\")\n",
    "data = []\n",
    "b2_labels = list()\n",
    "b2_layer_numbers = list()\n",
    "for f1, lab in zip(files, labels):\n",
    "    layer_num = re.search('Layer_(.+?).jpg', lab).group(1)\n",
    "    b2_layer_numbers.append(\"b2_\"+str(layer_num))\n",
    "    img = cv2.imread(f1)\n",
    "    b2_labels.append(int(lab[0]))\n",
    "    ######## Convert to Images to grey scale.\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    data.append(img)\n",
    "b2_images = np.array(data)\n",
    "print(b2_images[0].shape)\n",
    "print(b2_labels[0])\n",
    "print(b2_layer_numbers[0])\n",
    "(unique, counts) = np.unique(b2_labels, return_counts=True)\n",
    "print(unique, counts)\n",
    "print(b2_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d0a7e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 150, 3)\n",
      "0\n",
      "b3_1000\n",
      "[0 1] [898  65]\n",
      "(963, 190, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "files, labels = load_all_image_path(\"D:/UoH_PhD_Exp/Data/Crop_images/B3\")\n",
    "data = []\n",
    "b3_labels = list()\n",
    "b3_layer_numbers = list()\n",
    "for f1, lab in zip(files, labels):\n",
    "    layer_num = re.search('Layer_(.+?).jpg', lab).group(1)\n",
    "    b3_layer_numbers.append(\"b3_\"+str(layer_num))\n",
    "    img = cv2.imread(f1)\n",
    "    b3_labels.append(int(lab[0]))\n",
    "    ######## Convert to Images to grey scale.\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    data.append(img)\n",
    "b3_images = np.array(data)\n",
    "print(b3_images[0].shape)\n",
    "print(b3_labels[0])\n",
    "print(b3_layer_numbers[0])\n",
    "(unique, counts) = np.unique(b3_labels, return_counts=True)\n",
    "print(unique, counts)\n",
    "print(b3_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c2917",
   "metadata": {},
   "source": [
    "#### Concatenate all images datasets into one dataset\n",
    "\n",
    "X = Images     y = Image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d945fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (2889, 190, 150, 3)\n",
      "Total y: 2889\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((b1_images, b2_images, b3_images), axis=0)\n",
    "y = b1_labels + b2_labels + b3_labels\n",
    "layer_nums = b1_layer_numbers + b2_layer_numbers + b3_layer_numbers\n",
    "\n",
    "print(\"X Shape: \" + str(X.shape))\n",
    "print(\"Total y: \" + str(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bba7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ve_env",
   "language": "python",
   "name": "my_ve_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
